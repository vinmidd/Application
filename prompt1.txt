From Gemini

Okay, this is an excellent point to generate some initial code\! I'll craft a detailed prompt for Cursor IDE or GitHub Copilot, incorporating all the architectural decisions we've made, along with a suggested project folder structure.

-----

### **Part 1: Detailed Prompt for AI Code Generation**

**Prompt Title:** Initial Python Project Setup for Conversational AI Agent (APES PoC)

**Instructions for the AI Assistant:**

"You are an expert Python developer tasked with bootstrapping a conversational AI agent project. Based on the following requirements, generate the initial boilerplate code, including file structures, class definitions, function stubs, and a runnable example. Prioritize modularity, clear commenting, and adherence to the specified technologies.

-----

**Project Goal:**
Develop an AI-powered conversational agent (APES PoC) to automate Medicare ID card and Member Services inquiries by interacting with various backend non-production APIs.

**Core Technologies:**

  * **Python 3.9+**
  * **LangGraph:** For orchestrating the conversational flow as a stateful agentic loop.
  * **OpenAI:** For Large Language Model (LLM) capabilities (specifically, tool-calling models like `gpt-4o` or `gpt-3.5-turbo-1106+`).
  * **Pydantic:** For defining robust data models (e.g., `AgentState`).
  * **LangChain:** For `BaseTool` abstractions and `BaseMessage` types.
  * **SQLite:** For persistent conversation state management via `langgraph.checkpoint.sqlite.SQLiteSaver`.
  * **Flask:** For a minimal web-based chat interface.
  * **`python-dotenv`:** For managing API keys.

**Architectural Approach (LangGraph Agentic Loop):**

1.  **`AgentState` (Pydantic `BaseModel`):**

      * Central state object for the entire conversation.
      * Must contain `messages: List[BaseMessage]` to hold the conversation history (Human, AI, Tool messages).
      * Can optionally include other global state variables, e.g., `current_member_id: Optional[str]`.

2.  **Tools (API Wrappers as LangChain `@tool` functions):**

      * Each backend API will be wrapped in a Python function.
      * These functions must be decorated with `@tool` from `langchain_core.tools`.
      * Each tool function requires a clear, concise, and accurate `docstring` that serves as its `description` for the LLM. This description is critical for the LLM to understand when and how to use the tool.
      * Parameters for tool functions should use Python type hints and ideally be described in the docstring.
      * These tools will return strings (often JSON strings) representing the API response.
      * **Initial Tool Examples to Implement (as stubs that print API call info):**
          * `get_id_card_status(member_id: str, card_type: Optional[str] = None) -> str`: Retrieves ID card shipping status.
          * `request_new_id_card(member_id: str, reason: str) -> str`: Submits a request for a new ID card.
          * `get_member_benefits(member_id: str, plan_type: str) -> str`: Retrieves member benefits for a specific plan.
          * `get_dental_coverage_status(member_id: str) -> str`: Checks dental coverage for a member.
          * `get_member_status(member_id: str) -> str`: Gets general member status.
          * **Important:** Include a shared `api_client.py` module to encapsulate common API calling logic (e.g., base URL, headers, error handling), which the tool functions will import and use.

3.  **LangGraph Nodes:**

      * **`agent_node`:** A Python function that takes `AgentState` and `config`. It will initialize the LLM (bound with all available tools) and invoke it with `state.messages`. It returns an update to `AgentState` (typically an `AIMessage` with `tool_calls` or a direct `content` response).
      * **`tool_executor_node`:** This will be `langgraph.prebuilt.ToolNode` initialized with a list of all your LangChain `BaseTool` instances. It automatically executes `tool_calls` from the `AIMessage`.

4.  **LangGraph Routing:**

      * **Entry Point:** `agent_node`.
      * **Conditional Edge from `agent_node`:** A router function (`should_continue`) that inspects the last message from the `agent_node`.
          * If `last_message.tool_calls` exist, route to `tool_executor_node`.
          * Otherwise (LLM generated a direct response), route to `END`.
      * **Direct Edge from `tool_executor_node`:** Always route back to `agent_node` (for the LLM to process tool results and decide next steps).

5.  **State Persistence:**

      * Use `SQLiteSaver.from_conn_string("sqlite:///conversations.db")`.
      * Pass the `checkpointer` to `workflow.compile()`.
      * Ensure `thread_id` is passed in `config={"configurable": {"thread_id": "YOUR_UNIQUE_ID"}}` during `app_graph.invoke()`.

6.  **Flask Integration (Basic):**

      * A minimal Flask application (`app.py`).
      * A root route (`/`) to render an `index.html` with a simple chat interface (input field, message display area, submit button).
      * An API endpoint (`/chat`) that:
          * Receives user messages.
          * Retrieves/sets `thread_id` from session or a simple mechanism.
          * Loads existing `chat_history` for the `thread_id` (from session or a placeholder).
          * Initializes `AgentState` with the new message and history.
          * Invokes `app_graph.invoke()`.
          * Extracts the final AI response from the result.
          * Updates `chat_history` and sends the AI response back to the frontend.
      * Use Flask sessions to store `thread_id` and `chat_history` (for demonstration purposes; in production, `SQLiteSaver` would handle history automatically, but Flask session gives per-browser session context).

7.  **Environment Variables:**

      * Use `python-dotenv` to load `OPENAI_API_KEY` from a `.env` file.

**Code Generation Instructions:**

  * **Generate all necessary `import` statements.**
  * **Create the specified folder structure and empty `.py` files within them.**
  * **Fill `agent/state.py` with the `AgentState` Pydantic model.**
  * **Fill `agent/nodes.py` with the `call_agent` and `should_continue` functions.**
  * **Fill `agent/tools/api_client.py` with a basic shared `ApiClient` class or functions (stubs for actual HTTP calls).**
  * **Fill `agent/tools/id_card_tools.py` and `agent/tools/member_tools.py` with the `@tool` decorated stubs for the example APIs mentioned above.**
  * **Fill `agent/__init__.py` with code to dynamically collect all `@tool` functions from the `tools` sub-directory into a list called `all_tools`.**
  * **Fill `app.py` with:**
      * Flask app setup.
      * Loading environment variables.
      * LangGraph workflow definition (`StateGraph`, `add_node`, `add_conditional_edges`, `compile`).
      * Initialization of `ChatOpenAI` and `SQLiteSaver`.
      * The `/` and `/chat` Flask routes.
      * A basic `index.html` in the `templates/` folder.
  * **Fill `config.py` with placeholders for configuration variables (e.g., base URLs for APIs).**
  * **Generate a `requirements.txt` with all necessary packages.**
  * **Include a `.env.example` file.**
  * **Add clear comments to explain each section of the code.**

-----

### **Part 2: Suggested Folder Structure**

```
APES_Project/
├── .env.example
├── requirements.txt
├── app.py
├── config.py
├── conversations.db  <-- This file will be generated by SQLiteSaver
├── agent/
│   ├── __init__.py
│   ├── state.py
│   ├── nodes.py
│   └── tools/
│       ├── __init__.py         <-- To collect all @tool functions
│       ├── api_client.py       <-- Shared API client logic
│       ├── id_card_tools.py    <-- @tool functions for ID card APIs
│       └── member_tools.py     <-- @tool functions for Member APIs
│           # Add more tool files as needed (e.g., policy_tools.py, claims_tools.py)
├── templates/
│   └── index.html
└── static/
    ├── css/
    │   └── style.css  (Optional, for basic styling)
    └── js/
        └── script.js  (Optional, for frontend interaction)


---------

# This would typically be a string in your agent/nodes.py or config.py

SYSTEM_PROMPT = """You are a helpful and polite Medicare Assistant. Your primary goal is to assist users with their Medicare ID card and member-related inquiries by leveraging the tools at your disposal.

Here's how you should operate:

1.  **Understand the User's Request:** Carefully analyze the user's message and the conversation history to understand their core need.

2.  **Utilize Tools (APIs) When Necessary:**
    * You have access to a suite of specialized tools (functions that interact with Medicare backend systems).
    * **Always consider if a tool is needed to fulfill the user's request.** If information is required from the Medicare system or an action needs to be performed (like requesting a new ID card), you **must** use the appropriate tool.
    * **Prioritize using the most specific tool available** for the user's query.
    * **Extract all necessary parameters** for the chosen tool from the conversation context. If you cannot find a required parameter, ask the user for it clearly.
    * **Be precise with tool arguments.** Ensure the arguments you pass to the tool exactly match its schema and requirements.

3.  **Process Tool Outputs:**
    * After a tool executes, you will receive its output. Analyze this output carefully.
    * Use the tool's output to formulate a helpful and accurate response to the user.

4.  **Handle Missing Information/Clarification:**
    * If the user's query is ambiguous or if you lack a critical piece of information (e.g., a member ID) required by a tool, politely ask the user for clarification. State exactly what information you need.

5.  **Handle Out-of-Scope Queries:**
    * If a user's request is completely outside the scope of Medicare ID cards or member benefits (i.e., you don't have a tool or internal knowledge to answer it), politely state that you cannot assist with that specific request and offer to help with Medicare-related inquiries.

6.  **Generate User-Friendly Responses:**
    * Keep your responses clear, concise, and easy for the user to understand.
    * Maintain a polite and professional tone.
    * Do not share internal tool names or raw API responses directly with the user. Translate all information into natural, user-friendly language.
    * If multiple API calls are made, synthesize the information from all relevant tool outputs into a coherent answer.

7.  **Think Step-by-Step (Internal Monologue - for advanced LLMs):**
    * For complex queries, it can be helpful for you to internally "think" about your reasoning process before deciding on an action or response. (Some LLM models support a `reasoning` or `thought` field in their output when configured, which can be useful for debugging and fine-tuning.)

**Examples of when to use tools (and associated queries):**
* **`get_id_card_status`**: "What's the status of my ID card?", "Where is my new Medicare card?"
* **`request_new_id_card`**: "I need a new ID card.", "Can you send me a replacement card?"
* **`get_member_benefits`**: "What are the benefits of my plan?", "Does my plan cover X?"
* **`get_dental_coverage_status`**: "Is dental included in my Medicare plan?", "Do I have dental coverage?"
* **`get_member_status`**: "Am I an active member?", "What's my membership status?"

**Begin the conversation. I will provide the user's messages and any tool outputs.**
"""









------------------------------
You are an expert Python developer working on a PoC project called **APES (AI Powered Enterprise Services)**. Your goal is to implement a LangGraph-based conversational system that allows users to ask ID card–related questions in natural language.

Please generate all the necessary boilerplate and logic for the following:

---

📁 **Project Structure**
Create a modular project with these folders:

- `agents/`: contains `intent_resolver.py` and `response_agent.py`
- `tools/`: contains `track_id_card.py`, `estimate_id_card.py`, and `request_id_card.py`
- `orchestrator/`: contains `router.py` and `langgraph_flow.py`
- `memory/`: contains `memory_manager.py`
- `state/`: contains `graph_state.py`
- `mock_api/`: contains `mock_api_server.py` using FastAPI and sample data
- `config/`: contains `prompt_templates.py` and `settings.py`
- `test/`: contains unit tests for each module
- `ui/`: contains a **Flask-based chatbot UI**
- `main.py`: to invoke the LangGraph end-to-end flow

---

🧠 **Functional Flow**
User enters a query like:
> "I haven't received my ID card in 5 days, where is it?"

Your system should:
1. Use an LLM to **classify intent** (e.g., `track_id_card`, `estimate_id_card`, `request_id_card`)
2. Use memory to **retain context** (multi-turn)
3. Use an **orchestration layer** to select the correct tool
4. Call the correct **mock API tool** (track/estimate/request)
5. Use a **response agent** to generate human-readable replies

---

🖥️ **UI Requirements (Flask)**
- A simple Flask app (`ui/app.py`) that serves a web form
- Page should have:
  - An input box for user messages
  - A scrollable area that shows **conversation history** (previous user + bot messages)
- History should be maintained **in session or simple in-memory structure** for PoC
- On submission, the message should:
  - Trigger the LangGraph pipeline
  - Display the updated conversation in the same page
- Bootstrap / minimal styling is fine

---

📦 **LLM Calls Required**
- `intent_resolver.py`: Prompt to classify intent with confidence
- `response_agent.py`: Prompt to format API response for end-user
- Use `PydanticOutputParser` for both to parse outputs
- Optional: `entity_extraction.py` if you want structured slot filling

---

📐 **Schemas**
Please use Pydantic for all of the following models (put them in `schemas.py`):

- `GraphState`: shared LangGraph state
- `IntentResult`
- `EntityExtractionResult`
- `TrackCardInput`, `TrackCardResponse`
- `EstimateCardInput`, `EstimateCardResponse`
- `RequestCardInput`, `RequestCardResponse`
- `FinalBotResponse`

---

🔧 **Mock API**
Use FastAPI in `mock_api/mock_api_server.py` to simulate backend:
- `GET /idcard/status`
- `GET /plan/status`
- `POST /idcard/request`

Return mocked JSON values.

---

🧪 **Tests**
For each component, generate:
- Unit test for intent resolver
- Unit test for each tool
- End-to-end test simulating a full LangGraph run

---

📋 **Prompt Templates**
Create prompt templates in `config/prompt_templates.py` for:
- Intent classification
- Response generation
- Clarification/fallback

---

📚 **Tech Stack**
- Python 3.10+
- LangChain
- LangGraph
- Pydantic
- FastAPI
- **Flask (UI only)**
- OpenAI (gpt-4 or gpt-4o)

---

📌 Notes
- This is a **PoC only**. Do not implement production-grade error handling, logging, or authentication.
- Focus on **modular, clean code**, and **developer-friendly structure** for handoff.